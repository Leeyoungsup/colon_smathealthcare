{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import base64\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.swa_utils import AveragedModel, update_bn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as TF\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda\n",
    "from torchmetrics import MeanMetric\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML, clear_output\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params={'image_size':128,\n",
    "        'lr':2e-5,\n",
    "        'batch_size':8,\n",
    "        'epochs':10000,\n",
    "        'data_path':'../../data/dataset/colon_all/',\n",
    "        'n_step':1000}\n",
    "class ModelConfig:\n",
    "    BASE_CH = 128  # 128, 256, 256, 512, 512\n",
    "    BASE_CH_MULT = (1, 2, 2, 4, 4) # 128, 64, 32, 16, 8\n",
    "    APPLY_ATTENTION = (False, False, False, True, False)\n",
    "    DROPOUT_RATE = 0.1\n",
    "    TIME_EMB_MULT = 4 # 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader:\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "# def get_default_device():\n",
    "#     return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_num_parameter(model):\n",
    "    # Count the number of parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    return num_params\n",
    "\n",
    "def save_images(images, path, **kwargs):\n",
    "    grid = make_grid(images, **kwargs)\n",
    "    ndarr = grid.permute(1, 2, 0).to(\"cpu\").numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    im.save(path)\n",
    "    \n",
    "def get(element: torch.Tensor, t: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Get value at index position \"t\" in \"element\" and\n",
    "        reshape it to have the same dimension as a batch of images.\n",
    "    \"\"\"\n",
    "    ele = element.gather(-1, t)\n",
    "    return ele.reshape(-1, 1, 1, 1)\n",
    "\n",
    "def setup_log_directory(config):\n",
    "    '''Log and Model checkpoint directory Setup'''\n",
    "    \n",
    "    if os.path.isdir(config.root_log_dir):\n",
    "        # Get all folders numbers in the root_log_dir\n",
    "        folder_numbers = [int(folder.replace(\"version_\", \"\")) for folder in os.listdir(config.root_log_dir)]\n",
    "        \n",
    "        # Find the latest version number present in the log_dir\n",
    "        last_version_number = max(folder_numbers)\n",
    "\n",
    "        # New version name\n",
    "        version_name = f\"version_{last_version_number + 1}\"\n",
    "\n",
    "    else:\n",
    "        version_name = config.log_dir\n",
    "\n",
    "    # Update the training config default directory \n",
    "    log_dir        = os.path.join(config.root_log_dir,        version_name)\n",
    "    checkpoint_dir = os.path.join(config.root_checkpoint_dir, version_name)\n",
    "\n",
    "    # Create new directory for saving new experiment version\n",
    "    os.makedirs(log_dir,        exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Logging at: {log_dir}\")\n",
    "    print(f\"Model Checkpoint at: {checkpoint_dir}\")\n",
    "    \n",
    "    return log_dir, checkpoint_dir\n",
    "\n",
    "def frames2vid(images, save_path):\n",
    "\n",
    "    WIDTH = images[0].shape[1]\n",
    "    HEIGHT = images[0].shape[0]\n",
    "\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#     fourcc = 0\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(save_path, fourcc, 25, (WIDTH, HEIGHT))\n",
    "\n",
    "    # Appending the images to the video one by one\n",
    "    for image in images:\n",
    "        video.write(image)\n",
    "\n",
    "    # Deallocating memories taken for window creation\n",
    "    # cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    return\n",
    "\n",
    "def display_gif(gif_path):\n",
    "    b64 = base64.b64encode(open(gif_path, 'rb').read()).decode('ascii')\n",
    "    display(HTML(f'<img src=\"data:image/gif;base64,{b64}\" />'))\n",
    "\n",
    "def display_image(image):\n",
    "    plt.figure(figsize=(12, 6), facecolor='white')\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "def inverse_transform(tensors):\n",
    "    \"\"\"Convert tensors from [-1, 1] to [0, 255]\"\"\"\n",
    "    return ((tensors.clip(-1, 1) + 1.0) / 2.0) * 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512):\n",
    "        super().__init__()\n",
    "\n",
    "        half_dim = time_emb_dims // 2\n",
    "\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "\n",
    "        ts = torch.arange(total_time_steps, dtype=torch.float32)\n",
    "\n",
    "        emb = torch.unsqueeze(ts, dim=-1) * torch.unsqueeze(emb, dim=0)\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "\n",
    "        self.time_blocks = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(emb),\n",
    "            nn.Linear(in_features=time_emb_dims, out_features=time_emb_dims_exp),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(in_features=time_emb_dims_exp, out_features=time_emb_dims_exp),\n",
    "        )\n",
    "\n",
    "    def forward(self, time):\n",
    "        return self.time_blocks(time)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels=64):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_groups=8, num_channels=channels)\n",
    "        self.mhsa = nn.MultiheadAttention(embed_dim=self.channels, num_heads=4, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, H, W = x.shape\n",
    "        h = self.group_norm(x)\n",
    "        h = h.reshape(B, self.channels, H * W).swapaxes(1, 2)  # [B, C, H, W] --> [B, C, H * W] --> [B, H*W, C]\n",
    "        h, _ = self.mhsa(h, h, h)  # [B, H*W, C]\n",
    "        h = h.swapaxes(2, 1).view(B, self.channels, H, W)  # [B, C, H*W] --> [B, C, H, W]\n",
    "        return x + h\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, *, in_channels, out_channels, dropout_rate=0.1, time_emb_dims=512, apply_attention=False):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.act_fn = nn.SiLU()\n",
    "        # Group 1\n",
    "        self.normlize_1 = nn.GroupNorm(num_groups=8, num_channels=self.in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
    "\n",
    "        # Group 2 time embedding\n",
    "        self.dense_1 = nn.Linear(in_features=time_emb_dims, out_features=self.out_channels)\n",
    "\n",
    "        # Group 3\n",
    "        self.normlize_2 = nn.GroupNorm(num_groups=8, num_channels=self.out_channels)\n",
    "        self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
    "\n",
    "        if self.in_channels != self.out_channels:\n",
    "            self.match_input = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=1, stride=1)\n",
    "        else:\n",
    "            self.match_input = nn.Identity()\n",
    "\n",
    "        if apply_attention:\n",
    "            self.attention = AttentionBlock(channels=self.out_channels)\n",
    "        else:\n",
    "            self.attention = nn.Identity()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # group 1\n",
    "        h = self.act_fn(self.normlize_1(x))\n",
    "        h = self.conv_1(h)\n",
    "\n",
    "        # group 2\n",
    "        # add in timestep embedding\n",
    "        h += self.dense_1(self.act_fn(t))[:, :, None, None]\n",
    "\n",
    "        # group 3\n",
    "        h = self.act_fn(self.normlize_2(h))\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv_2(h)\n",
    "\n",
    "        # Residual and attention\n",
    "        h = h + self.match_input(x)\n",
    "        h = self.attention(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.downsample = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x, *args):\n",
    "        return self.downsample(x)\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=\"same\")\n",
    "        )\n",
    "\n",
    "    def forward(self, x, *args):\n",
    "        return self.upsample(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=3,\n",
    "        output_channels=3,\n",
    "        num_res_blocks=2,\n",
    "        base_channels=128,\n",
    "        base_channels_multiples=(1, 2, 4, 8),\n",
    "        apply_attention=(False, False, True, False),\n",
    "        dropout_rate=0.1,\n",
    "        time_multiple=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        time_emb_dims_exp = base_channels * time_multiple\n",
    "        self.time_embeddings = SinusoidalPositionEmbeddings(time_emb_dims=base_channels, time_emb_dims_exp=time_emb_dims_exp)\n",
    "\n",
    "        self.first = nn.Conv2d(in_channels=input_channels, out_channels=base_channels, kernel_size=3, stride=1, padding=\"same\")\n",
    "\n",
    "        num_resolutions = len(base_channels_multiples)\n",
    "\n",
    "        # Encoder part of the UNet. Dimension reduction.\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        curr_channels = [base_channels]\n",
    "        in_channels = base_channels\n",
    "\n",
    "        for level in range(num_resolutions):\n",
    "            out_channels = base_channels * base_channels_multiples[level]\n",
    "\n",
    "            for _ in range(num_res_blocks):\n",
    "\n",
    "                block = ResnetBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=apply_attention[level],\n",
    "                )\n",
    "                self.encoder_blocks.append(block)\n",
    "                \n",
    "                in_channels = out_channels\n",
    "                curr_channels.append(in_channels)\n",
    "\n",
    "            if level != (num_resolutions - 1):\n",
    "                self.encoder_blocks.append(DownSample(channels=in_channels))\n",
    "                curr_channels.append(in_channels)\n",
    "\n",
    "        # Bottleneck in between\n",
    "        self.bottleneck_blocks = nn.ModuleList(\n",
    "            (\n",
    "                ResnetBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=True,\n",
    "                ),\n",
    "                ResnetBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=False,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Decoder part of the UNet. Dimension restoration with skip-connections.\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "\n",
    "        for level in reversed(range(num_resolutions)):\n",
    "            out_channels = base_channels * base_channels_multiples[level]\n",
    "\n",
    "            for _ in range(num_res_blocks + 1):\n",
    "                encoder_in_channels = curr_channels.pop()\n",
    "                block = ResnetBlock(\n",
    "                    in_channels=encoder_in_channels + in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=apply_attention[level],\n",
    "                )\n",
    "\n",
    "                in_channels = out_channels\n",
    "                self.decoder_blocks.append(block)\n",
    "\n",
    "            if level != 0:\n",
    "                self.decoder_blocks.append(UpSample(in_channels))\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=8, num_channels=in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=\"same\"),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        time_emb = self.time_embeddings(t)\n",
    "\n",
    "        h = self.first(x)\n",
    "        outs = [h]\n",
    "\n",
    "        for layer in self.encoder_blocks:\n",
    "            h = layer(h, time_emb)\n",
    "            outs.append(h)\n",
    "\n",
    "        for layer in self.bottleneck_blocks:\n",
    "            h = layer(h, time_emb)\n",
    "\n",
    "        for layer in self.decoder_blocks:\n",
    "            if isinstance(layer, ResnetBlock):\n",
    "                out = outs.pop()\n",
    "                h = torch.cat([h, out], dim=1)\n",
    "            h = layer(h, time_emb)\n",
    "\n",
    "        h = self.final(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (converting each image into a tensor and normalizing between [-1, 1])\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Lambda(lambda x: (x - 0.5) * 2)]\n",
    ")\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, args,dataset):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.w = self.h = args['image_size']\n",
    "\n",
    "        # image and mask\n",
    "        self.image_path =glob(args['data_path']+dataset+'/image/*.png')\n",
    "        self.trans_1 = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((args['image_size'],args['image_size']), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "            ]\n",
    "        )\n",
    "    def trans(self,image_t,a):\n",
    "        image_t=F.to_tensor(F.rotate(self.trans_1(image_t),a))\n",
    "        return image_t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load image\n",
    "        image = Image.open(self.image_path[index])\n",
    "        # augment\n",
    "        angle=random.randint(0, 360)\n",
    "        \n",
    "        image = self.trans_1(F.to_tensor(image)) * 2.0 - 1.0\n",
    "        \n",
    "        return image\n",
    "    \n",
    "train_dataset=CustomDataset(params,'all')\n",
    "loader = DataLoader(train_dataset, params['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6), facecolor='white')\n",
    "\n",
    "for b_image in loader:\n",
    "    b_image = ((b_image.clip(-1, 1) + 1.0) / 2.0) * 255.0\n",
    "    grid_img = make_grid(b_image / 255.0, nrow=16, padding=True, pad_value=1, normalize=True)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_diffusion_timesteps=1000,\n",
    "        img_shape=(3, 64, 64),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_diffusion_timesteps = num_diffusion_timesteps\n",
    "        self.img_shape = img_shape\n",
    "                \n",
    "        # BETAs & ALPHAs required at different places in the Algorithm.\n",
    "        beta = self.get_betas()\n",
    "        \n",
    "        self.register_buffer('beta', beta)\n",
    "        self.register_buffer('alpha', 1 - beta)\n",
    "        self.register_buffer('sqrt_beta', torch.sqrt(beta))\n",
    "        self.register_buffer('alpha_cumulative', torch.cumprod(self.alpha, dim=0))\n",
    "        self.register_buffer('sqrt_alpha_cumulative', torch.sqrt(self.alpha_cumulative))\n",
    "        self.register_buffer('one_by_sqrt_alpha', 1. / torch.sqrt(self.alpha))\n",
    "        self.register_buffer('sqrt_one_minus_alpha_cumulative', torch.sqrt(1 - self.alpha_cumulative))\n",
    "\n",
    "        \n",
    "    def get_betas(self):\n",
    "        \"\"\"linear schedule, proposed in original ddpm paper\"\"\"\n",
    "        scale = 1000 / self.num_diffusion_timesteps\n",
    "        beta_start = scale * 1e-4\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(\n",
    "            beta_start,\n",
    "            beta_end,\n",
    "            self.num_diffusion_timesteps,\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x0: torch.Tensor, timesteps: torch.Tensor):\n",
    "        eps     = torch.randn_like(x0)  # Noise\n",
    "        mean    = get(self.sqrt_alpha_cumulative, t=timesteps) * x0  # Image scaled\n",
    "        std_dev = get(self.sqrt_one_minus_alpha_cumulative, t=timesteps) # Noise scaled\n",
    "        sample  = mean + std_dev * eps # scaled inputs * scaled noise\n",
    "\n",
    "        return sample, eps  # return ... , gt noise --> model predicts this)\n",
    "    \n",
    "    def reverse(self, x, ts, z, predicted_noise):\n",
    "        beta_t                            = get(self.beta, ts)\n",
    "        one_by_sqrt_alpha_t               = get(self.one_by_sqrt_alpha, ts)\n",
    "        sqrt_one_minus_alpha_cumulative_t = get(self.sqrt_one_minus_alpha_cumulative, ts) \n",
    "\n",
    "        x = (\n",
    "            one_by_sqrt_alpha_t\n",
    "            * (x - (beta_t / sqrt_one_minus_alpha_cumulative_t) * predicted_noise)\n",
    "            + torch.sqrt(beta_t) * z\n",
    "        )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    input_channels          = 3,\n",
    "    output_channels         = 3,\n",
    "    base_channels           = ModelConfig.BASE_CH,\n",
    "    base_channels_multiples = ModelConfig.BASE_CH_MULT,\n",
    "    apply_attention         = ModelConfig.APPLY_ATTENTION,\n",
    "    dropout_rate            = ModelConfig.DROPOUT_RATE,\n",
    "    time_multiple           = ModelConfig.TIME_EMB_MULT,\n",
    ").to(device)\n",
    "\n",
    "dataloader = DeviceDataLoader(loader, device=device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=params['lr'])\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "sd = SimpleDiffusion(\n",
    "    num_diffusion_timesteps = params['n_step'],\n",
    "    img_shape               = (3, params['image_size'], params['image_size']),\n",
    ").to(device)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x0s= next(iter(loader)).to(device)\n",
    "\n",
    "noisy_images = []\n",
    "specific_timesteps = [0, 10, 50, 100, 150, 200, 250, 300, 400, 600, 800, 999]\n",
    "\n",
    "for timestep in specific_timesteps:\n",
    "    timestep = torch.as_tensor(timestep, dtype=torch.long).to(device)\n",
    "    xts, _ = sd(x0s, timestep)\n",
    "    xts = make_grid(xts, nrow=1, padding=1)\n",
    "    \n",
    "    noisy_images.append(xts)\n",
    "\n",
    "_, ax = plt.subplots(1, len(noisy_images), figsize=(10, 5), facecolor='white')\n",
    "\n",
    "for i, (timestep, noisy_sample) in enumerate(zip(specific_timesteps, noisy_images)):\n",
    "    ax[i].imshow(noisy_sample.cpu().squeeze(0).permute(1, 2, 0))\n",
    "    ax[i].set_title(f't={timestep}',fontsize=8)\n",
    "    ax[i].axis('off')\n",
    "    ax[i].grid(False)\n",
    "\n",
    "plt.suptitle(\"Forward diffusion process\", y=0.9)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, sd, loader, optimizer, scaler, loss_fn, epoch=800, total_epochs=params['epochs'],\n",
    "                   base_config=params, training_config=params):\n",
    "    loss_record = MeanMetric()\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(total=len(loader), dynamic_ncols=True) as tq:\n",
    "        tq.set_description(f\"Train :: Epoch: {epoch}/{total_epochs}\")\n",
    "         \n",
    "        for x0s in loader:\n",
    "            tq.update(1)\n",
    "            \n",
    "            ts = torch.randint(low=1, high=training_config['n_step'], size=(x0s.shape[0],), device=device)\n",
    "            xts, gt_noise = sd(x0s, ts)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred_noise = model(xts, ts)\n",
    "                loss = loss_fn(gt_noise, pred_noise)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # scaler.unscale_(optimizer)\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            loss_value = loss.detach().item()\n",
    "            loss_record.update(loss_value)\n",
    "\n",
    "            tq.set_postfix_str(s=f\"Loss: {loss_value:.4f}\")\n",
    "\n",
    "        mean_loss = loss_record.compute().item()\n",
    "    \n",
    "        tq.set_postfix_str(s=f\"Epoch Loss: {mean_loss:.4f}\")\n",
    "    \n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def reverse_diffusion(model, sd, timesteps=1000, img_shape=(3, 64, 64), \n",
    "                      num_images=5, nrow=8, device=\"cpu\", generate_video=False, **kwargs):\n",
    "\n",
    "    x = torch.randn((num_images, *img_shape), device=device)\n",
    "    model.eval()\n",
    "\n",
    "    if kwargs.get(\"generate_video\", False):\n",
    "        outs = []\n",
    "\n",
    "    for time_step in tqdm(iterable=reversed(range(1, timesteps)), \n",
    "                          total=timesteps-1, dynamic_ncols=False, \n",
    "                          desc=\"Sampling :: \", position=0):\n",
    "\n",
    "        ts = torch.ones(num_images, dtype=torch.long, device=device) * time_step\n",
    "        z = torch.randn_like(x) if time_step > 1 else torch.zeros_like(x)\n",
    "\n",
    "        predicted_noise = model(x, ts)\n",
    "\n",
    "        beta_t                            = get(sd.beta, ts)\n",
    "        one_by_sqrt_alpha_t               = get(sd.one_by_sqrt_alpha, ts)\n",
    "        sqrt_one_minus_alpha_cumulative_t = get(sd.sqrt_one_minus_alpha_cumulative, ts) \n",
    "\n",
    "        x = (\n",
    "            one_by_sqrt_alpha_t\n",
    "            * (x - (beta_t / sqrt_one_minus_alpha_cumulative_t) * predicted_noise)\n",
    "            + torch.sqrt(beta_t) * z\n",
    "        )\n",
    "\n",
    "        if generate_video:\n",
    "            x_inv = inverse_transform(x).type(torch.uint8)\n",
    "            grid = make_grid(x_inv, nrow=nrow, pad_value=255.0).to(\"cpu\")\n",
    "            ndarr = torch.permute(grid, (1, 2, 0)).numpy()[:, :, ::-1]\n",
    "            outs.append(ndarr)\n",
    "\n",
    "    if generate_video: # Generate and save video of the entire reverse process. \n",
    "        frames2vid(outs, kwargs['save_path'])\n",
    "        display(Image.fromarray(outs[-1][:, :, ::-1])) # Display the image at the final timestep of the reverse process.\n",
    "\n",
    "    else: # Display and save the image at the final timestep of the reverse process. \n",
    "        x = inverse_transform(x).type(torch.uint8)\n",
    "        grid = make_grid(x, nrow=nrow, pad_value=255.0).to(\"cpu\")\n",
    "        pil_image = TF.functional.to_pil_image(grid)\n",
    "        if kwargs.get(\"save_path\", None):\n",
    "            pil_image.save(kwargs['save_path'], format=kwargs['save_path'].split(\".\")[-1].upper())\n",
    "        display_image(grid.permute(1, 2, 0))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 946/10000: 100%|██████████| 125/125 [00:39<00:00,  3.14it/s, Epoch Loss: 0.0239]\n",
      "Train :: Epoch: 947/10000: 100%|██████████| 125/125 [00:38<00:00,  3.21it/s, Epoch Loss: 0.0278]\n",
      "Train :: Epoch: 948/10000:  66%|██████▌   | 82/125 [00:24<00:12,  3.34it/s, Loss: 0.0078]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Algorithm 1: Training\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     11\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/colon/aa_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(epoch)\u001b[38;5;241m+\u001b[39mext\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, sd, loader, optimizer, scaler, loss_fn, epoch, total_epochs, base_config, training_config)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(loader), dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tq:\n\u001b[1;32m      7\u001b[0m     tq\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain :: Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x0s \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     10\u001b[0m         tq\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m         ts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, high\u001b[38;5;241m=\u001b[39mtraining_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_step\u001b[39m\u001b[38;5;124m'\u001b[39m], size\u001b[38;5;241m=\u001b[39m(x0s\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mDeviceDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Yield a batch of data after moving it to device\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl:\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m to_device(b, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# augment\u001b[39;00m\n\u001b[1;32m     29\u001b[0m angle\u001b[38;5;241m=\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m360\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrans_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_video = False\n",
    "ext = \".mp4\" if generate_video else \".png\"\n",
    "for epoch in range(1, params['epochs'] + 1):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Algorithm 1: Training\n",
    "    train_one_epoch(model, sd, dataloader, optimizer, scaler, loss_fn, epoch=epoch)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        save_path = '../../data/colon/aa_'+str(epoch)+ext\n",
    "        \n",
    "        # Algorithm 2: Sampling\n",
    "        reverse_diffusion(model, sd, timesteps=params['n_step'], num_images=32, generate_video=generate_video,\n",
    "            save_path=save_path, img_shape=(3,params['image_size'],params['image_size']), device=device,\n",
    "        )\n",
    "\n",
    "        # clear_output()\n",
    "        checkpoint_dict = {\n",
    "            \"opt\": optimizer.state_dict(),\n",
    "            \"scaler\": scaler.state_dict(),\n",
    "            \"model\": model.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint_dict, \"../../model/DDPM/model1.ckpt\")\n",
    "        del checkpoint_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_diffusion(model, sd, timesteps=params['n_step'], num_images=32, generate_video=generate_video,\n",
    "            save_path=save_path, img_shape=(3,params['image_size'],params['image_size']), device=device,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = set(['png', 'jpg', 'jpeg'])\n",
    "VIDEO_EXTENSIONS = set(['mp4', 'gif'])\n",
    "\n",
    "def generate(model, sd, num_images=128, timesteps=1000, nrow=16, format='png', device=\"cpu\"):\n",
    "    if format in IMG_EXTENSIONS:\n",
    "        generate_video = False\n",
    "    elif format in VIDEO_EXTENSIONS:\n",
    "        generate_video = True\n",
    "    else:\n",
    "        print('Unsupported format')\n",
    "        return\n",
    "\n",
    "    filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}.{format}\"\n",
    "\n",
    "    save_path = '../../data/colon/'+filename\n",
    "    reverse_diffusion(\n",
    "        model,\n",
    "        sd,\n",
    "        num_images=num_images,\n",
    "        generate_video=generate_video,\n",
    "        save_path=save_path,\n",
    "        timesteps=timesteps,\n",
    "        img_shape=(3,params['image_size'],params['image_size']),\n",
    "        nrow=nrow,\n",
    "        device=device\n",
    "    )\n",
    "    print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, sd, format='png', device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
