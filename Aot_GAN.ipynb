{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 08:59:14.324645: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-04 08:59:15.109602: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import model.aotgan \n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from loss1 import loss as loss_module\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"4\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':512,\n",
    "        'rates':[1, 2, 4, 8],\n",
    "        'block_num':8,\n",
    "        'model':'aotgan',\n",
    "        'gan_type':\"smgan\",\n",
    "        'lrg':2e-4,\n",
    "        'lrd':2e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':8,\n",
    "        'epochs':1000,\n",
    "        'data_path':'../../data/dataset/adenoma/',\n",
    "        'num_workers':4,\n",
    "        'rec_loss':'1*L1+250*Style+0.1*Perceptual'\n",
    "        }\n",
    "losses = list(params['rec_loss'].split(\"+\"))\n",
    "params['rec_loss'] = {}\n",
    "for l in losses:\n",
    "    weight, name = l.split(\"*\")\n",
    "    params['rec_loss'][name] = float(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, args,dataset):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.w = self.h = args['image_size']\n",
    "\n",
    "        # image and mask\n",
    "        self.image_path =glob(args['data_path']+dataset+'/image/*.jpg')\n",
    "        self.mask_path = [i.replace('/image','/mask') for i in self.image_path]\n",
    "        self.trans_1 = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((args['image_size'],args['image_size']), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "            ]\n",
    "        )\n",
    "    def trans(self,image_t,a):\n",
    "        image_t=F.to_tensor(F.rotate(self.trans_1(image_t),a))\n",
    "        return image_t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load image\n",
    "        image = Image.open(self.image_path[index]).convert(\"RGB\")\n",
    "        filename = os.path.basename(self.image_path[index])\n",
    "        mask = Image.open(self.mask_path[index])\n",
    "        mask = mask.convert(\"L\")\n",
    "        # augment\n",
    "        angle=random.randint(0, 360)\n",
    "        \n",
    "        image = self.trans(image,angle) * 2.0 - 1.0\n",
    "        mask = self.trans(mask,angle)\n",
    "        \n",
    "        return image, mask, filename\n",
    "    \n",
    "train_dataset=CustomDataset(params,'train')\n",
    "test_dataset=CustomDataset(params,'test')\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "         shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "         shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "netG =model.aotgan.InpaintGenerator(params).to(device)\n",
    "optimG = torch.optim.Adam(netG.parameters(), lr=params['lrg'], betas=(params['beta1'], params['beta2']))\n",
    "\n",
    "netD = model.aotgan.Discriminator().to(device)\n",
    "optimD = torch.optim.Adam(netD.parameters(), lr=params['lrd'], betas=(params['beta1'], params['beta2']))\n",
    "rec_loss_func = {key: getattr(loss_module, key)() for key, val in params['rec_loss'].items()}\n",
    "adv_loss = getattr(loss_module, \"smgan\")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1/1000 Step: 118 L1 loss : 0.2439 Style loss: 0.9957 Perceptual loss: 0.3158 advg loss: 0.0804 advd loss: 0.1069:  65%|██████▍   | 117/181 [01:41<00:55,  1.15it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(params['epochs']):\n",
    "    train=tqdm(train_dataloader)\n",
    "    count=0\n",
    "    train_L1_loss = 0.0 #예측된 값과 실제 값 사이의 절대값 차이의 평균 계산\n",
    "    train_Style_loss = 0.0 # 네트워크가 생성한 이미지가 목표 스타일 이미지와 비슷한 텍스처, 색상 분포 및 시각적 패턴 비교\n",
    "    train_Perceptual_loss = 0.0 #이미지의 전반적인 질감, 형태 및 콘텐츠의 유사성을 측정\n",
    "    train_advg_loss = 0.0\n",
    "    train_advd_loss = 0.0\n",
    "    sum_loss= 1000.0\n",
    "    for images, masks,filename in train:\n",
    "        count+=1\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        images_masked = (images * (1 - masks).float()) + masks\n",
    "        pred_img = netG(images_masked, masks)\n",
    "        comp_img = (1 - masks) * images + masks * pred_img\n",
    "        losses = {}\n",
    "        for name, weight in params['rec_loss'].items():\n",
    "            losses[name] = weight * rec_loss_func[name](pred_img, images)\n",
    "        dis_loss, gen_loss = adv_loss(netD, comp_img, images, masks)\n",
    "        losses[\"advg\"] = gen_loss*0.1\n",
    "        # backforward\n",
    "        optimG.zero_grad()\n",
    "        optimD.zero_grad()\n",
    "        sum(losses.values()).backward()\n",
    "        losses[\"advd\"] = dis_loss\n",
    "        dis_loss.backward()\n",
    "        optimG.step()\n",
    "        optimD.step()\n",
    "        train_L1_loss+=losses['L1'].item()\n",
    "        train_Style_loss+=losses['Style'].item()\n",
    "        train_Perceptual_loss+=losses['Perceptual'].item()\n",
    "        train_advg_loss+=losses['advg'].item()\n",
    "        train_advd_loss+=losses['advd'].item()\n",
    "        train.set_description(f\"epoch: {epoch+1}/{params['epochs']} Step: {count+1} L1 loss : {train_L1_loss/count:.4f} Style loss: {train_Style_loss/count:.4f} Perceptual loss: {train_Perceptual_loss/count:.4f} advg loss: {train_advg_loss/count:.4f} advd loss: {train_advd_loss/count:.4f}\")\n",
    "    if epoch % 10 ==5:\n",
    "        ax=plt.figure(figsize=(24,8))\n",
    "        ax.add_subplot(1,3,1)\n",
    "        plt.imshow(np.transpose(images[0].cpu().detach().numpy(),(1,2,0))/2+0.5)\n",
    "        ax.add_subplot(1,3,2)\n",
    "        plt.imshow(np.transpose(images_masked[0].cpu().detach().numpy(),(1,2,0))/2+0.5)\n",
    "        ax.add_subplot(1,3,3)\n",
    "        plt.imshow(np.transpose(pred_img[0].cpu().detach().numpy(),(1,2,0))/2+0.5)\n",
    "        plt.show()\n",
    "        \n",
    "    if sum_loss>sum(losses.values()):\n",
    "        sum_loss=sum(losses.values())\n",
    "        torch.save(netG.state_dict(), '../../model/aot-model_endoscopy/generator_check.pt')\n",
    "        torch.save(netD.state_dict(), '../../model/aot-model_endoscopy/discriminator_check.pt')   \n",
    "torch.save(netG.state_dict(), '../../model/aot-model_endoscopy/generator.pt')\n",
    "torch.save(netD.state_dict(), '../../model/aot-model_endoscopy/discriminator.pt')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
