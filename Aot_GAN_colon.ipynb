{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import model.aotgan \n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from loss1 import loss as loss_module\n",
    "\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':512,\n",
    "        'rates':[1, 2, 4, 8],\n",
    "        'block_num':8,\n",
    "        'model':'aotgan',\n",
    "        'gan_type':\"smgan\",\n",
    "        'lrg':2e-5,\n",
    "        'lrd':2e-5,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':8,\n",
    "        'epochs':10000,\n",
    "        'data_path':'../../data/dataset/colon/',\n",
    "        'num_workers':4,\n",
    "        'rec_loss':'1*L1+250*Style+0.1*Perceptual'\n",
    "        }\n",
    "losses = list(params['rec_loss'].split(\"+\"))\n",
    "params['rec_loss'] = {}\n",
    "for l in losses:\n",
    "    weight, name = l.split(\"*\")\n",
    "    params['rec_loss'][name] = float(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, args,dataset):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.w = self.h = args['image_size']\n",
    "\n",
    "        # image and mask\n",
    "        self.image_path =glob(args['data_path']+dataset+'/image/*.png')\n",
    "        self.mask_path = [i.replace('/image','/mask') for i in self.image_path]\n",
    "        self.trans_1 = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((args['image_size'],args['image_size']), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "            ]\n",
    "        )\n",
    "    def trans(self,image_t,a):\n",
    "        image_t=F.to_tensor(F.rotate(self.trans_1(image_t),a))\n",
    "        return image_t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load image\n",
    "        image = Image.open(self.image_path[index]).convert(\"RGB\")\n",
    "        filename = os.path.basename(self.image_path[index])\n",
    "        mask = Image.open(self.mask_path[index])\n",
    "        mask = mask.convert(\"L\")\n",
    "        # augment\n",
    "        angle=random.randint(0, 360)\n",
    "        \n",
    "        image = self.trans(image,angle) * 2.0 - 1.0\n",
    "        mask = self.trans(mask,angle)\n",
    "        \n",
    "        return image, mask, filename\n",
    "    \n",
    "train_dataset=CustomDataset(params,'train')\n",
    "test_dataset=CustomDataset(params,'test')\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "         shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "         shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG =model.aotgan.InpaintGenerator(params).to(device)\n",
    "optimG = torch.optim.Adam(netG.parameters(), lr=params['lrg'], betas=(params['beta1'], params['beta2']))\n",
    "\n",
    "netD = model.aotgan.Discriminator().to(device)\n",
    "optimD = torch.optim.Adam(netD.parameters(), lr=params['lrd'], betas=(params['beta1'], params['beta2']))\n",
    "rec_loss_func = {key: getattr(loss_module, key)() for key, val in params['rec_loss'].items()}\n",
    "adv_loss = getattr(loss_module, \"smgan\")()\n",
    "netG.load_state_dict(torch.load('../../model/aot-model_colon/generator_45.pt',map_location=device))\n",
    "netD.load_state_dict(torch.load('../../model/aot-model_colon/discriminator_45.pt',map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3112/10000 Step: 26 L1 loss : 0.0227 Style loss: 0.0335 Perceptual loss: 0.0504 advg loss: 0.0090 advd loss: 0.0004:  50%|█████     | 25/50 [00:39<00:39,  1.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m optimG\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m optimD\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 31\u001b[0m train_L1_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mlosses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m train_Style_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStyle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     33\u001b[0m train_Perceptual_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPerceptual\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plt_count=0\n",
    "sum_loss= 1000.0\n",
    "for epoch in range(params['epochs']):\n",
    "    train=tqdm(train_dataloader)\n",
    "    count=0\n",
    "    train_L1_loss = 0.0 #예측된 값과 실제 값 사이의 절대값 차이의 평균 계산\n",
    "    train_Style_loss = 0.0 # 네트워크가 생성한 이미지가 목표 스타일 이미지와 비슷한 텍스처, 색상 분포 및 시각적 패턴 비교\n",
    "    train_Perceptual_loss = 0.0 #이미지의 전반적인 질감, 형태 및 콘텐츠의 유사성을 측정\n",
    "    train_advg_loss = 0.0\n",
    "    train_advd_loss = 0.0\n",
    "    \n",
    "    for images, masks,filename in train:\n",
    "        count+=1\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        images_masked = (images * (1 - masks).float()) + masks\n",
    "        pred_img = netG(images_masked, masks)\n",
    "        comp_img = (1 - masks) * images + masks * pred_img\n",
    "        losses = {}\n",
    "        for name, weight in params['rec_loss'].items():\n",
    "            losses[name] = weight * rec_loss_func[name](pred_img, images)\n",
    "        dis_loss, gen_loss = adv_loss(netD, comp_img, images, masks)\n",
    "        losses[\"advg\"] = gen_loss*0.01\n",
    "        # backforward\n",
    "        optimG.zero_grad()\n",
    "        optimD.zero_grad()\n",
    "        sum(losses.values()).backward()\n",
    "        losses[\"advd\"] = dis_loss\n",
    "        dis_loss.backward()\n",
    "        optimG.step()\n",
    "        optimD.step()\n",
    "        train_L1_loss+=losses['L1'].item()\n",
    "        train_Style_loss+=losses['Style'].item()\n",
    "        train_Perceptual_loss+=losses['Perceptual'].item()\n",
    "        train_advg_loss+=losses['advg'].item()\n",
    "        train_advd_loss+=losses['advd'].item()\n",
    "        train.set_description(f\"epoch: {epoch+1}/{params['epochs']} Step: {count+1} L1 loss : {train_L1_loss/count:.4f} Style loss: {train_Style_loss/count:.4f} Perceptual loss: {train_Perceptual_loss/count:.4f} advg loss: {train_advg_loss/count:.4f} advd loss: {train_advd_loss/count:.4f}\")\n",
    "    if epoch % 50 ==5:\n",
    "        torch.save(netG.state_dict(), '../../model/aot-model_colon/generator_'+str(plt_count)+'.pt')\n",
    "        torch.save(netD.state_dict(), '../../model/aot-model_colon/discriminator_'+str(plt_count)+'.pt') \n",
    "        plt_count+=1\n",
    "        ax=plt.figure(figsize=(24,8))\n",
    "        ax.add_subplot(1,3,1)\n",
    "        plt.imshow(np.transpose(images[0].cpu().detach().numpy(),(1,2,0))/2+0.5)\n",
    "        ax.add_subplot(1,3,2)\n",
    "        plt.imshow(np.transpose(images_masked[0].cpu().detach().numpy(),(1,2,0))/2+0.5)\n",
    "        ax.add_subplot(1,3,3)\n",
    "        plt.imshow(np.transpose(pred_img[0].cpu().detach().numpy(),(1,2,0))/2+0.5)\n",
    "        plt.show()\n",
    "        print(str(plt_count)+'_epoch')\n",
    "    if sum_loss>sum(losses.values()):\n",
    "        sum_loss=sum(losses.values())\n",
    "        torch.save(netG.state_dict(), '../../model/aot-model_colon/generator_check.pt')\n",
    "        torch.save(netD.state_dict(), '../../model/aot-model_colon/discriminator_check.pt')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
