{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model.mat as MAT\n",
    "import os\n",
    "from glob import glob\n",
    "import model.aotgan \n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from loss1 import loss as loss_module\n",
    "from torch_utils import misc\n",
    "import copy\n",
    "from torch_utils.ops import conv2d_gradfix\n",
    "from losses.pcp import PerceptualLoss\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':512,\n",
    "        'rates':[1, 2, 4, 8],\n",
    "        'block_num':8,\n",
    "        'model':'aotgan',\n",
    "        'gan_type':\"smgan\",\n",
    "        'lrg':2e-4,\n",
    "        'lrd':2e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':4,\n",
    "        'epochs':10000,\n",
    "        'data_path':'../../data/dataset/colon/',\n",
    "        'num_workers':4,\n",
    "        'rec_loss':'1*L1+100*Style+0.1*Perceptual'\n",
    "        }\n",
    "losses = list(params['rec_loss'].split(\"+\"))\n",
    "params['rec_loss'] = {}\n",
    "for l in losses:\n",
    "    weight, name = l.split(\"*\")\n",
    "    params['rec_loss'][name] = float(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, args,dataset):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.w = self.h = args['image_size']\n",
    "\n",
    "        # image and mask\n",
    "        self.image_path =glob(args['data_path']+dataset+'/image/*.png')\n",
    "        self.mask_path = [i.replace('/image','/mask') for i in self.image_path]\n",
    "        self.trans_1 = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((args['image_size'],args['image_size']), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "            ]\n",
    "        )\n",
    "    def trans(self,image_t,a):\n",
    "        image_t=F.to_tensor(F.rotate(self.trans_1(image_t),a))\n",
    "        return image_t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load image\n",
    "        image = Image.open(self.image_path[index]).convert(\"RGB\")\n",
    "        filename = os.path.basename(self.image_path[index])\n",
    "        mask = Image.open(self.mask_path[index])\n",
    "        mask = mask.convert(\"L\")\n",
    "        # augment\n",
    "        angle=random.randint(0, 360)\n",
    "        \n",
    "        image = self.trans(image,angle) * 2.0 - 1.0\n",
    "        mask = self.trans(mask,angle)\n",
    "        \n",
    "        return image, mask, filename\n",
    "    \n",
    "train_dataset=CustomDataset(params,'train')\n",
    "test_dataset=CustomDataset(params,'test')\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "         shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=params['batch_size'],\n",
    "         shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "netG = MAT.Generator(z_dim=512, c_dim=0, w_dim=512, img_resolution=512, img_channels=3).to(device)\n",
    "netD = MAT.Discriminator(c_dim=0, img_resolution=params['image_size'], img_channels=3).to(device)\n",
    "optimG = torch.optim.Adam(netG.parameters(), lr=params['lrg'], betas=(params['beta1'], params['beta2']))\n",
    "optimD = torch.optim.Adam(netD.parameters(), lr=params['lrd'], betas=(params['beta1'], params['beta2']))\n",
    "G_ema = copy.deepcopy(netG).eval()\n",
    "G_mapping=netG.mapping\n",
    "truncation_psi=1\n",
    "style_mixing_prob=0.9\n",
    "G_synthesis=netG.synthesis\n",
    "pcp = PerceptualLoss(layer_weights=dict(conv4_4=1/4, conv5_4=1/2)).to(device)\n",
    "def run_G(img_in, mask_in, z, c, sync):\n",
    "    with misc.ddp_sync(G_mapping, sync):\n",
    "        ws = G_mapping(z, c, truncation_psi=truncation_psi)\n",
    "        if style_mixing_prob > 0:\n",
    "            with torch.autograd.profiler.record_function('style_mixing'):\n",
    "                cutoff = torch.empty([], dtype=torch.int64, device=ws.device).random_(1, ws.shape[1])\n",
    "                cutoff = torch.where(torch.rand([], device=ws.device) < style_mixing_prob, cutoff, torch.full_like(cutoff, ws.shape[1]))\n",
    "                ws[:, cutoff:] = G_mapping(torch.randn_like(z), c, truncation_psi=truncation_psi, skip_w_avg_update=True)[:, cutoff:]\n",
    "    with misc.ddp_sync(G_synthesis, sync):\n",
    "        img, img_stg1 = G_synthesis(img_in, mask_in, ws, return_stg1=True)\n",
    "    return img, ws, img_stg1\n",
    "\n",
    "def run_D(img, mask, img_stg1, c, sync):\n",
    "    # if augment_pipe is not None:\n",
    "    #     # img = augment_pipe(img)\n",
    "    #     # !!!!! have to remove the color transform\n",
    "    #     tmp_img = torch.cat([img, mask], dim=1)\n",
    "    #     tmp_img = augment_pipe(tmp_img)\n",
    "    #     img, mask = torch.split(tmp_img, [3, 1])\n",
    "    with misc.ddp_sync(netD, sync):\n",
    "        logits, logits_stg1 = netD(img, mask, img_stg1, c)\n",
    "    return logits, logits_stg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1957/10000 Step: 101 D loss : 1.3863 G loss: 1.4897 : 100%|██████████| 100/100 [02:38<00:00,  1.59s/it]\n",
      "epoch: 1958/10000 Step: 101 D loss : 1.3863 G loss: 1.4898 : 100%|██████████| 100/100 [02:35<00:00,  1.56s/it]\n",
      "epoch: 1959/10000 Step: 101 D loss : 1.3863 G loss: 1.4895 : 100%|██████████| 100/100 [02:43<00:00,  1.64s/it]\n",
      "epoch: 1960/10000 Step: 101 D loss : 1.3863 G loss: 1.4894 : 100%|██████████| 100/100 [02:41<00:00,  1.62s/it]\n",
      "epoch: 1961/10000 Step: 101 D loss : 1.3863 G loss: 1.4892 : 100%|██████████| 100/100 [02:47<00:00,  1.68s/it]\n",
      "epoch: 1962/10000 Step: 101 D loss : 1.3863 G loss: 1.4891 : 100%|██████████| 100/100 [02:38<00:00,  1.59s/it]\n",
      "epoch: 1963/10000 Step: 101 D loss : 1.3863 G loss: 1.4896 : 100%|██████████| 100/100 [02:36<00:00,  1.56s/it]\n",
      "epoch: 1964/10000 Step: 101 D loss : 1.3863 G loss: 1.4899 : 100%|██████████| 100/100 [02:38<00:00,  1.59s/it]\n",
      "epoch: 1965/10000 Step: 101 D loss : 1.3863 G loss: 1.4891 : 100%|██████████| 100/100 [02:51<00:00,  1.71s/it]\n",
      "epoch: 1966/10000 Step: 101 D loss : 1.3863 G loss: 1.4897 : 100%|██████████| 100/100 [02:42<00:00,  1.62s/it]\n",
      "epoch: 1967/10000 Step: 101 D loss : 1.3863 G loss: 1.4893 : 100%|██████████| 100/100 [02:46<00:00,  1.66s/it]\n",
      "epoch: 1968/10000 Step: 101 D loss : 1.3863 G loss: 1.4893 : 100%|██████████| 100/100 [02:34<00:00,  1.54s/it]\n",
      "epoch: 1969/10000 Step: 101 D loss : 1.3863 G loss: 1.4892 : 100%|██████████| 100/100 [02:38<00:00,  1.59s/it]\n",
      "epoch: 1970/10000 Step: 101 D loss : 1.3863 G loss: 1.4897 : 100%|██████████| 100/100 [02:36<00:00,  1.57s/it]\n",
      "epoch: 1971/10000 Step: 101 D loss : 1.3863 G loss: 1.4890 : 100%|██████████| 100/100 [02:45<00:00,  1.65s/it]\n",
      "epoch: 1972/10000 Step: 101 D loss : 1.3863 G loss: 1.4898 : 100%|██████████| 100/100 [02:41<00:00,  1.61s/it]\n",
      "epoch: 1973/10000 Step: 15 D loss : 1.3863 G loss: 1.5219 :  14%|█▍        | 14/100 [00:24<02:31,  1.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m train_Dr_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \n\u001b[1;32m      9\u001b[0m sum_loss\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000.0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks,filename \u001b[38;5;129;01min\u001b[39;00m train:\n\u001b[1;32m     11\u001b[0m     count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m     images, masks \u001b[38;5;241m=\u001b[39m (images\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), masks\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     24\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_path[index])\n\u001b[1;32m     25\u001b[0m mask \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_path[index])\n\u001b[0;32m---> 26\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# augment\u001b[39;00m\n\u001b[1;32m     28\u001b[0m angle\u001b[38;5;241m=\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m360\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/PIL/Image.py:922\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    891\u001b[0m     mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     colors: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image:\n\u001b[1;32m    897\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;124;03m    omitted, a mode is chosen so that all information in the image\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03m    and the palette can be represented without a palette.\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \n\u001b[1;32m    903\u001b[0m \u001b[38;5;124;03m    This supports all possible conversions between \"L\", \"RGB\" and \"CMYK\". The\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    ``matrix`` argument only supports \"L\" and \"RGB\".\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \n\u001b[1;32m    906\u001b[0m \u001b[38;5;124;03m    When translating a color image to grayscale (mode \"L\"),\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;124;03m    the library uses the ITU-R 601-2 luma transform::\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        L = R * 299/1000 + G * 587/1000 + B * 114/1000\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \n\u001b[1;32m    911\u001b[0m \u001b[38;5;124;03m    The default method of converting a grayscale (\"L\") or \"RGB\"\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;124;03m    image into a bilevel (mode \"1\") image uses Floyd-Steinberg\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m    dither to approximate the original image luminosity levels. If\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    dither is ``None``, all values larger than 127 are set to 255 (white),\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;124;03m    all other values to 0 (black). To use other thresholds, use the\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m    :py:meth:`~PIL.Image.Image.point` method.\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \n\u001b[1;32m    918\u001b[0m \u001b[38;5;124;03m    When converting from \"RGBA\" to \"P\" without a ``matrix`` argument,\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m    this passes the operation to :py:meth:`~PIL.Image.Image.quantize`,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;124;03m    and ``dither`` and ``palette`` are ignored.\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;124;03m    When converting from \"PA\", if an \"RGBA\" palette is present, the alpha\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;124;03m    channel from the image will be used instead of the values from the palette.\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \n\u001b[1;32m    925\u001b[0m \u001b[38;5;124;03m    :param mode: The requested mode. See: :ref:`concept-modes`.\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;124;03m    :param matrix: An optional conversion matrix.  If given, this\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m       should be 4- or 12-tuple containing floating point values.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m    :param dither: Dithering method, used when converting from\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;124;03m       mode \"RGB\" to \"P\" or from \"RGB\" or \"L\" to \"1\".\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;124;03m       Available methods are :data:`Dither.NONE` or :data:`Dither.FLOYDSTEINBERG`\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;124;03m       (default). Note that this is not used when ``matrix`` is supplied.\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03m    :param palette: Palette to use when converting from mode \"RGB\"\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;124;03m       to \"P\".  Available palettes are :data:`Palette.WEB` or\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m       :data:`Palette.ADAPTIVE`.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03m    :param colors: Number of colors to use for the :data:`Palette.ADAPTIVE`\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03m       palette. Defaults to 256.\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;124;03m    :rtype: :py:class:`~PIL.Image.Image`\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    943\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/PIL/ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "plt_count=0\n",
    "for epoch in range(params['epochs']):\n",
    "    train=tqdm(train_dataloader)\n",
    "    count=0\n",
    "    train_Dgen_loss = 0.0 #예측된 값과 실제 값 사이의 절대값 차이의 평균 계산\n",
    "    train_Ggen_loss = 0.0 # 네트워크가 생성한 이미지가 목표 스타일 이미지와 비슷한 텍스처, 색상 분포 및 시각적 패턴 비교\n",
    "    train_Dr_loss = 0.0 \n",
    "    \n",
    "    sum_loss= 1000.0\n",
    "    for images, masks,filename in train:\n",
    "        count+=1\n",
    "        images, masks = (images-0.5).to(device), masks.to(device)\n",
    "        z = torch.randn(params['batch_size'], 512).to(device)\n",
    "        c = torch.randn(params['batch_size'], 512).to(device)\n",
    "        masks=1-masks\n",
    "        images_masked = (images * (masks).float()) + (1-masks)\n",
    "        optimG.zero_grad()\n",
    "        optimD.zero_grad()\n",
    "        with torch.autograd.profiler.record_function('Gmain_forward'):\n",
    "            gen_img, _gen_ws, gen_img_stg1 = run_G(images, masks,z, c, sync=True) # May get synced by Gpl.\n",
    "            gen_logits, gen_logits_stg1 = run_D(images, masks, gen_img,c, sync=False)\n",
    "            \n",
    "            # dis_loss, gen_loss = adv_loss(netD, comp_img, images, masks)\n",
    "            # losses[\"advg\"] = gen_loss\n",
    "            # # backforward\n",
    "            loss_Gmain = torch.nn.functional.softplus(-gen_logits)\n",
    "            loss_Gmain_stg1 = torch.nn.functional.softplus(-gen_logits_stg1)\n",
    "            pcp_loss, _ = pcp(gen_img, images)\n",
    "        with torch.autograd.profiler.record_function('Gmain_backward'):\n",
    "            loss_Gmain_all = loss_Gmain + loss_Gmain_stg1 + pcp_loss\n",
    "            loss_Gmain_all.mean().mul(1).backward()      \n",
    "        # losses[\"advd\"] = dis_loss\n",
    "        # dis_loss.backward()\n",
    "        optimG.step()\n",
    "        with torch.autograd.profiler.record_function('Dgen_forward'):\n",
    "            gen_img, _gen_ws, gen_img_stg1 = run_G(images, masks, z, None, sync=False)\n",
    "            gen_logits, gen_logits_stg1 = run_D(images, masks, gen_img, None, sync=False) # Gets synced by loss_Dreal.\n",
    "            loss_Dgen_stg1 = torch.nn.functional.softplus(gen_logits_stg1)\n",
    "            loss_Dgen = torch.nn.functional.softplus(gen_logits) # -log(1 - sigmoid(gen_logits))\n",
    "        with torch.autograd.profiler.record_function('Dgen_backward'):\n",
    "            loss_Dgen_all = loss_Dgen + loss_Dgen_stg1\n",
    "            loss_Dgen_all.mean().mul(1).backward()\n",
    " \n",
    "        \n",
    "       \n",
    "        optimD.step()\n",
    "        \n",
    "        train_Dgen_loss+=loss_Dgen_all.mean().mul(1).item()\n",
    "        train_Ggen_loss+=loss_Gmain_all.mean().mul(1).item()\n",
    "        train.set_description(f\"epoch: {epoch+1}/{params['epochs']} Step: {count+1} D loss : {train_Dgen_loss/count:.4f} G loss: {train_Ggen_loss/count:.4f} \")\n",
    "    if epoch % 50 ==5:\n",
    "        torch.save(netG.state_dict(), '../../model/MAT_colon/generator_'+str(plt_count)+'.pt')\n",
    "        torch.save(netD.state_dict(), '../../model/MAT_colon/discriminator_'+str(plt_count)+'.pt') \n",
    "        plt_count+=1\n",
    "        ax=plt.figure(figsize=(12,4))\n",
    "        ax.add_subplot(1,3,1)\n",
    "        plt.imshow(np.transpose((images[0]+0.5).cpu().detach().numpy(),(1,2,0))/2+0.5)\n",
    "        ax.add_subplot(1,3,2)\n",
    "        plt.imshow(np.transpose((images_masked[0]+0.5).cpu().detach().numpy(),(1,2,0))/2+0.5)\n",
    "        ax.add_subplot(1,3,3)\n",
    "        plt.imshow(np.transpose((gen_img[0]+0.5).cpu().detach().numpy(),(1,2,0))/2+0.5)\n",
    "\n",
    "        plt.show()\n",
    "        print(str(plt_count)+'_epoch')\n",
    "    if sum_loss>(train_Dgen_loss/count+train_Ggen_loss/count):\n",
    "        sum_loss=train_Dgen_loss/count+train_Ggen_loss/count\n",
    "        torch.save(netG.state_dict(), '../../model/MAT_colon/generator_check.pt')\n",
    "        torch.save(netD.state_dict(), '../../model/MAT_colon/discriminator_check.pt')   \n",
    "torch.save(netG.state_dict(), '../../model/MAT_colon/generator.pt')\n",
    "torch.save(netD.state_dict(), '../../model/MAT_colon/discriminator.pt')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
